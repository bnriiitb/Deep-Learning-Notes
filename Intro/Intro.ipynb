{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning\n",
    "### Table of Contents\n",
    "<p>\n",
    "    <div class=\"lev1 toc-item\">\n",
    "        <a href=\"## Why Deep Learning and Why Now?\" data-toc-modified-id=\"## Why Deep Learning and Why Now?\">\n",
    "            <span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Why Deep Learning and Why Now?\n",
    "        </a>\n",
    "    </div>\n",
    "</p>\n",
    "\n",
    "## 1. Why Deep Learning and Why Now?\n",
    "\n",
    "### 1.1. Why Deep Learning\n",
    "* In traditional `Machine Learning`, the `features are hand engineered` this approach is time consuming, brittle and not scalable.\n",
    "* The key idea of `Deep Learning` is to learn the underlying patterns directly from data.\n",
    "* Ex: Learning features like lines, edges from the image data.\n",
    "\n",
    "### 1.2. Why Now?\n",
    "* The `Neural Networks` are dated back decades but due to reasons mentioned below it's gained a lot of momentum at the moment.\n",
    "    1. Big Data\n",
    "        * Larger Datasets\n",
    "        * Easier for collection and storage\n",
    "    2. Hardware\n",
    "        * GPU (Graphics Processing Units)\n",
    "        * Massively Parallelizable\n",
    "    3. Software\n",
    "        * Improved Techniques\n",
    "        * New Models\n",
    "        * Toolboxes\n",
    "\n",
    "## 2. What is the fundamental building block of a Neural Network?\n",
    "* A Neuron is the fundamental building block of a Neural Network.\n",
    "* A Neuron is also called as the `Perceptron`.\n",
    "* The Perceptron is the structural building block of deep learning.\n",
    "\n",
    "<img src='images/perceptron.png'>\n",
    "\n",
    "* The idea of Perceptron is very simple\n",
    "* Forward Propogation of infromation through a Neuron\n",
    "    1. Define a set of `Inputs` to that Neuron : $X_{1}$,...,$X_{m}$\n",
    "    2. Each of these inputs have a corresponding `Weight`: $W_{1}$,...,$W_{m}$\n",
    "    3. With each of these inputs and weights, multiply them correspondingly together and take a sum $\\sum$ of all them.\n",
    "    4. Take the summation value and pass it throught what is called a `Non-Linear Activation` function and that produces the final output $\\hat{y}$\n",
    "* We usually have what's called a `Bias` term in this Nueron.\n",
    "<img src='images/perceptron_with_bias_term.png'>\n",
    "\n",
    "### 2.1. What is the purpose of a Bias Term?\n",
    "* The `Bias` term shifts the activation function either towards left or right regardless of the input values.\n",
    "* So, the `Bias` term is not affected by the input ${X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = {g}\\big{(} w_{0} + \\sum^{m}_{i=1} x_{i} w_{i}\\big{)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above summation equation can be rewritten using linear algebra in terms of vectors and dot products.\n",
    "\n",
    "$\\hat{y} = g(w_{0}+X^{T}W)\\\\[1em]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "where: X = \\begin{bmatrix}\n",
    "X_{1} \\\\.\\\\.\\\\.\\\\ X_{m}\n",
    "\\end{bmatrix},\n",
    "W = \\begin{bmatrix}\n",
    "W_{1}\\\\.\\\\.\\\\.\\\\W_{m}\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, to compute the output of a single perceptron we need to take the \n",
    "    1. Dot Product of $X^{T}$ and $W$ which represents the elementwise multiplication and Summation\n",
    "    2. Apply the Non-Linearity\n",
    "\n",
    "<img src='images/perceptron_with_activation_func.png'>\n",
    "\n",
    "* Sigmoid: Outputs the probability of each class\n",
    "\n",
    "<img src='images/activation_func.png'>\n",
    "\n",
    "### 2.2. What is the Importance of an Activation Function?\n",
    "* The purpose of activation function is to introduce non-linearities into the network\n",
    "* Linear Activation function produce linear decisions no matter the network size.\n",
    "* Non-linearities allow us to approximate arbitrarily complex functions.\n",
    "<img src='images/activate_fun_ex.png'>\n",
    "<img src='images/activation_func_ex2.png'>\n",
    "\n",
    "<img src='images/perceptron_example.png'>\n",
    "<hr>\n",
    "\n",
    "<img src='images/perceptron_example_1.png'>\n",
    "\n",
    "<img src='images/perceptron_ex_2.png'>\n",
    "\n",
    "## 3. Building Neural Networks with Perceptrons\n",
    "\n",
    "<img src='images/perceptron_simplified.png'>\n",
    "<hr>\n",
    "\n",
    "<img src='images/ps_1.png'>\n",
    "<hr>\n",
    "\n",
    "### Multi Output Perceptron\n",
    "* Because all the inputs are densely connected to all outputs, these layers are called as `Dense` layers.\n",
    "<img src='images/ps_2.png'>\n",
    "\n",
    "### Build a Dense Layer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(MyDenseLayer,self).__init__()\n",
    "        # initialize weights and biases\n",
    "        self.W=self.add_weight([input_dim,output_dim])\n",
    "        self.b=self.add_weight([1,output_dim])\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        # forward propogate the inputs\n",
    "        z = tf.matmul(inputs,self.W)+self.b\n",
    "        # feed through a non-linear activation\n",
    "        output = tf.math.sigmoid()\n",
    "        return output\n",
    "\n",
    "```\n",
    "We can achieve the same by usin below code\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "layer = tf.keras.layers.Dense(units=2)\n",
    "```\n",
    "\n",
    "### Single Layer Neural Network\n",
    "<img src='images/single_layer_nn.png'>\n",
    "<img src='images/single_layer_nn1.png'>\n",
    "\n",
    "### Multi Output Perceptron\n",
    "<img src='images/multi_output_perc.png'>\n",
    "\n",
    "#### Code to Implement it:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "```\n",
    "\n",
    "### Deep Neural Network\n",
    "<img src='images/deep_nn.png'>\n",
    "\n",
    "#### Code to implement:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n1),\n",
    "    tf.keras.layers.Dense(n2),\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "## 4. Applying Neural Networks\n",
    "\n",
    "#### Example Problem: Will I pass this class?\n",
    "\n",
    "Let's start with a simple 2 feature model\n",
    "* $x_{1}$ = Number of lectures attended\n",
    "* $x_{2}$ = Hours spent on final project\n",
    "<img src='images/ex_problem.png'>\n",
    "\n",
    "<img src='images/ex_problem1.png'>\n",
    "\n",
    "* When we pass on the data $x^{(1)}=[4,5]$ the perceptron just outputs 0.1 though the actual value is $1$. Because the network is not trained.\n",
    "\n",
    "### Quantifying Loss\n",
    "* The loss of our network measures the cost incurred from incorrect predictions.\n",
    "<img src='images/q_loss.png'>\n",
    "\n",
    "### Empirical Loss / Objective Function / Cost Function / Empirical Risk\n",
    "<img src='images/el.png'>\n",
    "\n",
    "* The example problem that we are discussing is a Binary Classification problem.\n",
    "* In Binary Classification task we use `Binary Cross Entropy Loss` as the loss function.\n",
    "### Binary Cross Entropy Loss\n",
    "* Compares the predicted vs. actual values distribution the difference would be the loss from actual to predicted.\n",
    "<img src='images/bcel.png'>\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,predicted))\n",
    "```\n",
    "\n",
    "* Instead of predicting whether a student will pass or fail, if we want to predict the students grade/score. \n",
    "* The task would become a Regression Problem, hence the loss function we use also change.\n",
    "\n",
    "### Mean Squared Error Loss\n",
    "<img src='images/mser.png'>\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y,predicted)))\n",
    "```\n",
    "\n",
    "## 5. Training Neural Networks\n",
    "\n",
    "### Loss Optimization\n",
    "* We want to find the network wights that achieve the lowest loss.\n",
    "<img src='images/lo.png'>\n",
    "<img src='images/lo1.png'>\n",
    "\n",
    "### Gradient Descent\n",
    "<img src='images/gd.png'>\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "weights = tf.Variable([tf.random.normal()])\n",
    "while True: # loop forever\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = compute_loss(weights)\n",
    "        gradient = g.gradient(loss,weights)\n",
    "    weights = weights-lr*gradient\n",
    "```    \n",
    "\n",
    "### Computing Gradients: Back Propogation\n",
    "<img src='images/bp.png'>\n",
    "<img src='images/bp1.png'>\n",
    "<img src='images/bp2.png'>\n",
    "\n",
    "## 6. Neural Networks in Practice: Optimization\n",
    "* Training a Neural Network Can be difficult because Loss Functions can be difficult to optimize.\n",
    "<img src='images/ogd.png'>\n",
    "\n",
    "### How can we set the learning rate?\n",
    "* Small learning rate converges slowly and gets stuck in false local minima.\n",
    "* Large learning rates overshoot, become unstable and diverge.\n",
    "* Stable learning rate converges smoothly and avoid local minima.\n",
    "\n",
    "### How to deal with this?\n",
    "#### Idea 1:\n",
    "* Try lots of different learning rates and see what works **just right**\n",
    "\n",
    "#### Idea 2:\n",
    "* Do something smarter\n",
    "* Design an adaptive learning rate that **adapts** to the landscape.\n",
    "\n",
    "### Adaptive Learning Rates\n",
    "* Learnng rates are no longer fixed\n",
    "* Can be made larger or smaller depending on:\n",
    "    * how large gradient is\n",
    "    * how fast learning is happening\n",
    "    * size of particular weights\n",
    "    \n",
    "### Gradient Descent Algorithms\n",
    "<img src='images/gda.png'>\n",
    "\n",
    "## Putting it all together\n",
    "```python\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([...])\n",
    "# pick your favorite optimizer\n",
    "optimizer = tf.keras.optimizer.SGD()\n",
    "while True:\n",
    "    # forward pass through the network\n",
    "    prediction = model(x)\n",
    "    with tf.GradientTape() as tape:\n",
    "        # compute the loss\n",
    "        loss = compute_loss(y,prediction)\n",
    "  # update the weights using the gradient\n",
    "  grads = tape.gradient(loss,model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads,model.traianble_variables)))\n",
    "```\n",
    "\n",
    "## 7. Neural Networks in Practice: Mini-Batches\n",
    "\n",
    "* Gradient Descent is computationally very intensive to compute.\n",
    "* Computing Gradient Descent for each data point is noisy.\n",
    "* So, to mitigate this what we do is that instead of computing gradient for all the points, we  compute for a batch of points.\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "* Fast to compute and much better estimate of the true gradient\n",
    "\n",
    "<img src='images/sgd.png'>\n",
    "\n",
    "\n",
    "### Mini-batches while training\n",
    "\n",
    "* More accurate estimation of gradient\n",
    "    * Smoother convergence\n",
    "    * Allows for large learning rates\n",
    "* Mini-batches lead to fast training\n",
    "    * Can parallelize computation + achieve significant speed increases on GPU's\n",
    "\n",
    "## 8. Nueral Networks in Practice : Overfitting\n",
    "* Overfitting : Problem of generalization\n",
    "\n",
    "### The Problem of Overfitting\n",
    "<img src='images/over_fitting.png'>\n",
    "\n",
    "* To handle overfitting in Neural Networks we use rely on a couple of techniques:\n",
    "* Regularization\n",
    "\n",
    "### Regularization\n",
    "* What is it?\n",
    "    * Regularization is a technique that constraints our optimization problem to discourage learning complex models\n",
    "* Why do we need it?\n",
    "    * This improves generalization of our model on unseen data\n",
    "* Most popular technique of Regularization in Deep Learning are:\n",
    "    * Dropout.\n",
    "    * Early Stopping\n",
    "\n",
    "#### Dropout\n",
    "* During training randomly set some activations to 0.\n",
    "    * Typically drop 50% of activations in a layer.\n",
    "    * Forces network to not rely on any 1 node.\n",
    "\n",
    "<img src='images/drop_out.png'>\n",
    "\n",
    "#### Early Stopping\n",
    "* Stop trainig before we have a chance to overfit.\n",
    "<img src='images/es.png'>\n",
    "\n",
    "## Core Foundation Review\n",
    "<img src='images/foundation_review.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
